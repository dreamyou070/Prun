
 step 2. noise scheduler and solver
 (1) teacher pipe
Loading pipeline components...:  50%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                              | 3/6 [00:01<00:02,  1.47it/s]/home/dreamyou070/.conda/envs/venv_prun3/lib/python3.9/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.
  warnings.warn(
Loading pipeline components...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:02<00:00,  2.55it/s]
 (2) student pipe
Loading pipeline components...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:02<00:00,  2.23it/s]
 (3.2) student model
 (3.3) sub models

 step 3. optimizer
It seems like you have activated model offloading by calling `enable_model_cpu_offload`, but are now manually moving the pipeline to GPU. It is strongly recommended against doing so as memory gains from offloading are likely to be lost. Offloading automatically takes care of moving the individual components vae, text_encoder, tokenizer, unet, motion_adapter, scheduler, feature_extractor, image_encoder to GPU when needed. To make sure offloading works as expected, you should consider moving the pipeline back to CPU: `pipeline.to('cpu')` or removing the move altogether if you use offloading.

 step 4. target unet
 step 5. weight and device
 final weight_dtype : torch.float32
 step 6. move to device
 step 7. Enable optimizations
loading annotations from /scratch2/dreamyou070/MyData/video/panda/test_sample_trimmed/sample.csv ...
/home/dreamyou070/Prun/tests/distill.py:397: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Steps:   0%|                                                                                                                                                                                                                                             | 1/50000 [00:07<110:44:57,  7.97s/it]Traceback (most recent call last):
student_unet device : cuda:0
noisy_model_input device : cuda:0
start_timesteps device : cuda:0
prompt_embeds device : cuda:0
[2024-11-05 00:05:39,578] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/dreamyou070/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
  File "/home/dreamyou070/Prun/tests/distill.py", line 624, in <module>
    main(args)
  File "/home/dreamyou070/Prun/tests/distill.py", line 518, in main
    save_model = accelerator.unwrap_model(student_unet)
  File "/home/dreamyou070/.conda/envs/venv_prun3/lib/python3.9/site-packages/accelerate/accelerator.py", line 2628, in unwrap_model
    return extract_model_from_parallel(model, keep_fp32_wrapper)
  File "/home/dreamyou070/.conda/envs/venv_prun3/lib/python3.9/site-packages/accelerate/utils/other.py", line 86, in extract_model_from_parallel
    from deepspeed import DeepSpeedEngine
  File "/home/dreamyou070/.local/lib/python3.9/site-packages/deepspeed/__init__.py", line 25, in <module>
    from . import ops
  File "/home/dreamyou070/.local/lib/python3.9/site-packages/deepspeed/ops/__init__.py", line 11, in <module>
    from . import transformer
  File "/home/dreamyou070/.local/lib/python3.9/site-packages/deepspeed/ops/transformer/__init__.py", line 7, in <module>
    from .inference.config import DeepSpeedInferenceConfig
  File "/home/dreamyou070/.local/lib/python3.9/site-packages/deepspeed/ops/transformer/inference/__init__.py", line 8, in <module>
    from .moe_inference import DeepSpeedMoEInferenceConfig, DeepSpeedMoEInference
  File "/home/dreamyou070/.local/lib/python3.9/site-packages/deepspeed/ops/transformer/inference/moe_inference.py", line 17, in <module>
    from ....moe.sharded_moe import TopKGate
  File "/home/dreamyou070/.local/lib/python3.9/site-packages/deepspeed/moe/sharded_moe.py", line 28, in <module>
    from .mappings import drop_tokens, gather_tokens
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 982, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 925, in _find_spec
  File "<frozen importlib._bootstrap_external>", line 1423, in find_spec
  File "<frozen importlib._bootstrap_external>", line 1395, in _get_spec
  File "<frozen importlib._bootstrap_external>", line 1555, in find_spec
  File "<frozen importlib._bootstrap_external>", line 156, in _path_isfile
  File "<frozen importlib._bootstrap_external>", line 148, in _path_is_mode_type
  File "<frozen importlib._bootstrap_external>", line 142, in _path_stat
KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/dreamyou070/Prun/tests/distill.py", line 624, in <module>
[rank0]:     main(args)
[rank0]:   File "/home/dreamyou070/Prun/tests/distill.py", line 518, in main
[rank0]:     save_model = accelerator.unwrap_model(student_unet)
[rank0]:   File "/home/dreamyou070/.conda/envs/venv_prun3/lib/python3.9/site-packages/accelerate/accelerator.py", line 2628, in unwrap_model
[rank0]:     return extract_model_from_parallel(model, keep_fp32_wrapper)
[rank0]:   File "/home/dreamyou070/.conda/envs/venv_prun3/lib/python3.9/site-packages/accelerate/utils/other.py", line 86, in extract_model_from_parallel
[rank0]:     from deepspeed import DeepSpeedEngine
[rank0]:   File "/home/dreamyou070/.local/lib/python3.9/site-packages/deepspeed/__init__.py", line 25, in <module>
[rank0]:     from . import ops
[rank0]:   File "/home/dreamyou070/.local/lib/python3.9/site-packages/deepspeed/ops/__init__.py", line 11, in <module>
[rank0]:     from . import transformer
[rank0]:   File "/home/dreamyou070/.local/lib/python3.9/site-packages/deepspeed/ops/transformer/__init__.py", line 7, in <module>
[rank0]:     from .inference.config import DeepSpeedInferenceConfig
[rank0]:   File "/home/dreamyou070/.local/lib/python3.9/site-packages/deepspeed/ops/transformer/inference/__init__.py", line 8, in <module>
[rank0]:     from .moe_inference import DeepSpeedMoEInferenceConfig, DeepSpeedMoEInference
[rank0]:   File "/home/dreamyou070/.local/lib/python3.9/site-packages/deepspeed/ops/transformer/inference/moe_inference.py", line 17, in <module>
[rank0]:     from ....moe.sharded_moe import TopKGate
[rank0]:   File "/home/dreamyou070/.local/lib/python3.9/site-packages/deepspeed/moe/sharded_moe.py", line 28, in <module>
[rank0]:     from .mappings import drop_tokens, gather_tokens
[rank0]:   File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
[rank0]:   File "<frozen importlib._bootstrap>", line 982, in _find_and_load_unlocked
[rank0]:   File "<frozen importlib._bootstrap>", line 925, in _find_spec
[rank0]:   File "<frozen importlib._bootstrap_external>", line 1423, in find_spec
[rank0]:   File "<frozen importlib._bootstrap_external>", line 1395, in _get_spec
[rank0]:   File "<frozen importlib._bootstrap_external>", line 1555, in find_spec
[rank0]:   File "<frozen importlib._bootstrap_external>", line 156, in _path_isfile
[rank0]:   File "<frozen importlib._bootstrap_external>", line 148, in _path_is_mode_type
[rank0]:   File "<frozen importlib._bootstrap_external>", line 142, in _path_stat
[rank0]: KeyboardInterrupt
